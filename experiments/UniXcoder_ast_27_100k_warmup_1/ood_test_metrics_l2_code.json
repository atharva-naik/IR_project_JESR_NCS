{"CoNaLa": {"avg_candidate_rank": 10.86, "avg_best_candidate_rank": 5.235616438356164, "recall": {"@5": 0.696, "@10": 0.828, "@15": 0.884, "@20": 0.91, "@25": 0.93, "@30": 0.938, "@35": 0.948, "@40": 0.952, "@45": 0.956, "@50": 0.958}, "mrr": 0.5822568699144648, "ndcg": 0.6829760912346658}, "External Knowledge": {"avg_candidate_rank": 4.908653846153846, "avg_best_candidate_rank": 3.4575342465753423, "recall": {"@5": 0.8990384615384616, "@10": 0.9375, "@15": 0.9591346153846154, "@20": 0.96875, "@25": 0.9759615384615384, "@30": 0.9783653846153846, "@35": 0.9783653846153846, "@40": 0.9783653846153846, "@45": 0.9783653846153846, "@50": 0.9783653846153846}, "mrr": 0.820055672509444, "ndcg": 0.8612323014232585}, "Web Query": {"avg_candidate_rank": 15.549713193116634, "avg_best_candidate_rank": 7.418738049713193, "recall": {"@5": 0.5908221797323135, "@10": 0.7428298279158699, "@15": 0.7973231357552581, "@20": 0.8365200764818356, "@25": 0.869980879541109, "@30": 0.8891013384321224, "@35": 0.8996175908221797, "@40": 0.9091778202676865, "@45": 0.9177820267686424, "@50": 0.9235181644359465}, "mrr": 0.4711860688863819, "ndcg": 0.6267938790388068}}