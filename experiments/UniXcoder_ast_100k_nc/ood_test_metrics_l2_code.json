{"CoNaLa": {"avg_candidate_rank": 25.166, "avg_best_candidate_rank": 16.731506849315068, "recall": {"@5": 0.5, "@10": 0.616, "@15": 0.688, "@20": 0.76, "@25": 0.792, "@30": 0.816, "@35": 0.836, "@40": 0.858, "@45": 0.868, "@50": 0.876}, "mrr": 0.39403851546547014, "ndcg": 0.5276001553211199}, "External Knowledge": {"avg_candidate_rank": 16.014423076923077, "avg_best_candidate_rank": 15.498630136986302, "recall": {"@5": 0.7259615384615384, "@10": 0.8052884615384616, "@15": 0.8461538461538461, "@20": 0.8533653846153846, "@25": 0.875, "@30": 0.8894230769230769, "@35": 0.8990384615384616, "@40": 0.9086538461538461, "@45": 0.9158653846153846, "@50": 0.9230769230769231}, "mrr": 0.619993759423754, "ndcg": 0.6994135186282008}, "Web Query": {"avg_candidate_rank": 24.666347992351817, "avg_best_candidate_rank": 10.831739961759082, "recall": {"@5": 0.51434034416826, "@10": 0.6520076481835564, "@15": 0.7246653919694073, "@20": 0.7734225621414914, "@25": 0.8097514340344169, "@30": 0.8279158699808795, "@35": 0.8499043977055449, "@40": 0.8680688336520076, "@45": 0.878585086042065, "@50": 0.8919694072657743}, "mrr": 0.4206688837168216, "ndcg": 0.5824245132130438}}