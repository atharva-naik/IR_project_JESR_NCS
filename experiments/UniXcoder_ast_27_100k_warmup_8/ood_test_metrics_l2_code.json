{"CoNaLa": {"avg_candidate_rank": 7.05, "avg_best_candidate_rank": 4.6, "recall": {"@5": 0.71, "@10": 0.848, "@15": 0.882, "@20": 0.922, "@25": 0.94, "@30": 0.95, "@35": 0.958, "@40": 0.962, "@45": 0.97, "@50": 0.974}, "mrr": 0.5962055410175143, "ndcg": 0.694956849222468}, "External Knowledge": {"avg_candidate_rank": 4.3701923076923075, "avg_best_candidate_rank": 2.9506849315068493, "recall": {"@5": 0.90625, "@10": 0.9399038461538461, "@15": 0.9567307692307693, "@20": 0.9663461538461539, "@25": 0.96875, "@30": 0.9711538461538461, "@35": 0.9759615384615384, "@40": 0.9759615384615384, "@45": 0.9807692307692307, "@50": 0.9855769230769231}, "mrr": 0.8385602784493265, "ndcg": 0.8756505578356555}, "Web Query": {"avg_candidate_rank": 15.722753346080307, "avg_best_candidate_rank": 7.588910133843212, "recall": {"@5": 0.595602294455067, "@10": 0.7418738049713193, "@15": 0.8154875717017208, "@20": 0.8546845124282982, "@25": 0.8833652007648184, "@30": 0.8996175908221797, "@35": 0.9101338432122371, "@40": 0.9235181644359465, "@45": 0.9282982791586998, "@50": 0.9340344168260039}, "mrr": 0.49589660271157415, "ndcg": 0.646835574892123}}