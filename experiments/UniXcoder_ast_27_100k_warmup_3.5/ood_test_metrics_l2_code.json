{"CoNaLa": {"avg_candidate_rank": 7.962, "avg_best_candidate_rank": 5.136986301369863, "recall": {"@5": 0.688, "@10": 0.822, "@15": 0.87, "@20": 0.91, "@25": 0.928, "@30": 0.94, "@35": 0.958, "@40": 0.966, "@45": 0.97, "@50": 0.974}, "mrr": 0.5745731133067112, "ndcg": 0.6777364737134901}, "External Knowledge": {"avg_candidate_rank": 5.271634615384615, "avg_best_candidate_rank": 3.813698630136986, "recall": {"@5": 0.8990384615384616, "@10": 0.9375, "@15": 0.9567307692307693, "@20": 0.96875, "@25": 0.96875, "@30": 0.9735576923076923, "@35": 0.9735576923076923, "@40": 0.9735576923076923, "@45": 0.9735576923076923, "@50": 0.9735576923076923}, "mrr": 0.8296902816299297, "ndcg": 0.8683937676517894}, "Web Query": {"avg_candidate_rank": 17.48661567877629, "avg_best_candidate_rank": 9.175908221797323, "recall": {"@5": 0.6080305927342257, "@10": 0.748565965583174, "@15": 0.8126195028680688, "@20": 0.8499043977055449, "@25": 0.8776290630975143, "@30": 0.8891013384321224, "@35": 0.8967495219885278, "@40": 0.9082217973231358, "@45": 0.9235181644359465, "@50": 0.9311663479923518}, "mrr": 0.4937062627093276, "ndcg": 0.6443158993603056}}